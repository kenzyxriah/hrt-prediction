{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üìù Assignment: Modeling & Evaluation  \n\n## üöÄ Modelling  \nIn this section, you will build multiple machine learning models and compare their performance.  \n\n### Tasks:  \n1. **Train a Logistic Regression model** and validate it using a **classification report**.  \n2. **Train a Decision Tree model** and validate it using a **classification report**.  \n3. **Train a Random Forest model** and validate it using a **classification report**.  \n4. **Compare the models** using a **confusion matrix** and **classification report** to determine the best-performing model.  \n5. **Perform hyperparameter tuning** for each model to improve performance. Use `cross_val_score()` on `X_scaled` and `y` to validate results.  \n\n## üìå Conclusion  \n- Identify the **best-performing model** based on evaluation metrics.  \n- Specify the **best hyperparameters** used for the top model.  \n\n## üì§ Submission Instructions  \n1. **Create a prediction file** using the test dataset with your best-performing model and submit it to **Kaggle**.  \n2. **Upload your complete notebook** (including training, evaluation, and predictions) to your **team's GitHub repository**.  \n\nHappy coding! üöÄ  \n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}